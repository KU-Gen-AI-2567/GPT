{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KU-Gen-AI-2567/GPT/blob/main/GPT_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gpvcd-dWuZg"
      },
      "source": [
        "# GPT - 01418496\n",
        "**à¸ªà¸¡à¸²à¸Šà¸´à¸à¸à¸¥à¸¸à¹ˆà¸¡**\n",
        "\n",
        "à¸™à¸²à¸¢à¸¨à¸´à¸§à¸à¸£ à¸ à¸²à¸ªà¸§à¹ˆà¸²à¸‡ 6410451423\n",
        "\n",
        "à¸™à¸²à¸‡à¸ªà¸²à¸§ à¹€à¹€à¸à¸£à¸§à¸£à¸¸à¹‰à¸‡ à¸à¸¸à¸”à¸Šà¸°à¸§à¸² 6410451253\n",
        "\n",
        "à¸™à¸²à¸‡à¸ªà¸²à¸§ à¸¡à¸²à¸£à¸µà¸™à¹ˆà¸² à¸¡à¸´à¸—à¸‹à¸¸à¸¢ 6410450222\n",
        "\n",
        "à¸«à¸¡à¸¹à¹ˆ 200\n",
        "\n",
        "à¸Šà¸¸à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ : Disneyland Reviews\n",
        "\n",
        "à¸¥à¸´à¹‰à¸‡à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸” : https://www.kaggle.com/datasets/arushchillar/disneyland-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TJ99SV3bWuZh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import kagglehub\n",
        "import shutil\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrUf4zRWuZh"
      },
      "source": [
        "## Setting to execute on Processor (GPU or CPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChVjtilkWuZi",
        "outputId": "a7c75b64-d405-4073-c3f4-7f17be386614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execute on GPU\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if len(gpus) > 0:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    print(\"Execute on GPU\")\n",
        "else:\n",
        "    print(\"Execute on CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij-RDzKpWuZi"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cso5wPROWuZi",
        "outputId": "6cb7dea5-f06a-4db5-8748-83e1240373b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/arushchillar/disneyland-reviews?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.1M/11.1M [00:00<00:00, 69.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/arushchillar/disneyland-reviews/versions/1\n",
            "Download Dataset Complete\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset folder in latest version\n",
        "if not \"dataset\" in os.listdir(\".\"):\n",
        "    path = kagglehub.dataset_download(\"arushchillar/disneyland-reviews\")\n",
        "    print(\"Path to dataset files:\", path)\n",
        "    shutil.move(path, \"./dataset\")\n",
        "    print(\"Download Dataset Complete\")\n",
        "else:\n",
        "    print(\"Download Dataset Already\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KyZCEc7WuZi"
      },
      "source": [
        "## Prepossessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asz8mrRCWuZi",
        "outputId": "470a5fa4-54e3-4c56-d84d-27eb75491872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Review_ID  Rating Year_Month     Reviewer_Location  \\\n",
            "0  670772142       4     2019-4             Australia   \n",
            "1  670682799       4     2019-5           Philippines   \n",
            "2  670623270       4     2019-4  United Arab Emirates   \n",
            "3  670607911       4     2019-4             Australia   \n",
            "4  670607296       4     2019-4        United Kingdom   \n",
            "\n",
            "                                         Review_Text               Branch  \n",
            "0  If you've ever been to Disneyland anywhere you...  Disneyland_HongKong  \n",
            "1  Its been a while since d last time we visit HK...  Disneyland_HongKong  \n",
            "2  Thanks God it wasn   t too hot or too humid wh...  Disneyland_HongKong  \n",
            "3  HK Disneyland is a great compact park. Unfortu...  Disneyland_HongKong  \n",
            "4  the location is not in the city, took around 1...  Disneyland_HongKong  \n"
          ]
        }
      ],
      "source": [
        "file_path = \"./dataset/DisneylandReviews.csv\"\n",
        "df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2kpq-WJeIHs",
        "outputId": "8d0f418a-8825-4ff5-ad6d-5b320afffe4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      reviewer_location               branch  rating  \\\n",
            "0             Australia  Disneyland_HongKong       4   \n",
            "1           Philippines  Disneyland_HongKong       4   \n",
            "2  United Arab Emirates  Disneyland_HongKong       4   \n",
            "3             Australia  Disneyland_HongKong       4   \n",
            "4        United Kingdom  Disneyland_HongKong       4   \n",
            "\n",
            "                                         review_text  \n",
            "0  If you've ever been to Disneyland anywhere you...  \n",
            "1  Its been a while since d last time we visit HK...  \n",
            "2  Thanks God it wasn   t too hot or too humid wh...  \n",
            "3  HK Disneyland is a great compact park. Unfortu...  \n",
            "4  the location is not in the city, took around 1...  \n"
          ]
        }
      ],
      "source": [
        "df_selected = df[[\"Reviewer_Location\", \"Branch\", \"Rating\", \"Review_Text\"]]\n",
        "df_selected = df_selected.rename(columns=str.lower)\n",
        "print(df_selected.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TmtK_TIcGi8",
        "outputId": "6b9ec2b4-8e5d-4569-f5e5-28e671194e51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'reviewer_location': 'Australia',\n",
              " 'branch': 'Disneyland_HongKong',\n",
              " 'rating': 4,\n",
              " 'review_text': \"If you've ever been to Disneyland anywhere you'll find Disneyland Hong Kong very similar in the layout when you walk into main street! It has a very familiar feel. One of the rides  its a Small World  is absolutely fabulous and worth doing. The day we visited was fairly hot and relatively busy but the queues moved fairly well. \"}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data_list = df_selected.to_dict(orient=\"records\")\n",
        "data_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEZZAiU-dNvb"
      },
      "source": [
        "### Sequence construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fET05xSadTP8",
        "outputId": "b953b936-1f26-411f-91b7-2f7d57c8a412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42656 recipes loaded\n",
            "Disneyland review : United States : Disneyland_HongKong : 5 : Disneyland never cease to amaze me! I've been to Disneyland florida and I thought I have exhausted the kid in me but nope! I still had so much fun in disneyland hong kong. 2 DL off my bucketlist and more to come!     \n"
          ]
        }
      ],
      "source": [
        "filtered_data = [\n",
        "    \"Disneyland review : \"\n",
        "    + x[\"reviewer_location\"]\n",
        "    + \" : \"\n",
        "    + x[\"branch\"]\n",
        "    + \" : \"\n",
        "    + str(x[\"rating\"])\n",
        "    + \" : \"\n",
        "    + x[\"review_text\"]\n",
        "\n",
        "    for x in data_list\n",
        "    if x[\"reviewer_location\"] is not None\n",
        "    and x[\"branch\"] is not None\n",
        "    and x[\"rating\"] is not None\n",
        "    and x[\"review_text\"] is not None\n",
        "]\n",
        "\n",
        "n_data = len(filtered_data)\n",
        "print(f\"{n_data} recipes loaded\")\n",
        "\n",
        "example = filtered_data[10]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjC6LOBohl6q"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8ObIm4LRhj3T",
        "outputId": "76b85b2b-c6c5-452b-8018-ee2bb927883d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Disneyland review : United States : Disneyland_HongKong : 5 : Disneyland never cease to amaze me ! I've been to Disneyland florida and I thought I have exhausted the kid in me but nope ! I still had so much fun in disneyland hong kong . 2 DL off my bucketlist and more to come ! \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def pad_punctuation(s):\n",
        "    s = re.sub(r\"([^\\w\\s'-])\", r\" \\1 \", s) # à¹„à¸¡à¹ˆà¹à¸¢à¸ _ , -\n",
        "    s = re.sub(\" +\", \" \", s)\n",
        "    return s\n",
        "\n",
        "text_data = [pad_punctuation(x) for x in filtered_data]\n",
        "\n",
        "example_data = text_data[10]\n",
        "example_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB0UxOi-jtiK",
        "outputId": "507db895-2031-4659-b91a-44d77adb286f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \n",
            "1: [UNK]\n",
            "2: .\n",
            "3: the\n",
            "4: :\n",
            "5: and\n",
            "6: ,\n",
            "7: to\n",
            "8: a\n",
            "9: of\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "BATCH_SIZE = 32 # ğŸ¤”\n",
        "VOCAB_SIZE = 10000 # ğŸ¤”\n",
        "MAX_LEN = 80 # ğŸ¤”\n",
        "\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(text_data)\n",
        "text_ds = text_ds.batch(BATCH_SIZE)\n",
        "text_ds = text_ds.shuffle(1000)\n",
        "\n",
        "\n",
        "vectorize_layer = layers.TextVectorization( # ğŸ¤”\n",
        "    standardize=\"lower\", # ğŸ¤”\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LEN + 1,\n",
        ")\n",
        "\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "    print(f\"{i}: {word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpoG-W8Uk8L3",
        "outputId": "8f51fe8f-f0d6-4718-b87f-22a72ccccd14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  14   22    4   45   64    4  104    4   39    4   14  195    1    7\n",
            " 4191  158   19  426   91    7   14  246    5   17  349   17   34 1631\n",
            "    3  431   11  158   21 4345   19   17  125   44   35   89   98   11\n",
            "   14  234  235    2   80 1006  205   42    1    5   68    7  221   19\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "example_tokenised = vectorize_layer(example_data)\n",
        "print(example_tokenised.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfTCL9Bhk-9u"
      },
      "source": [
        "### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAEmiBW4lC8v",
        "outputId": "7f6a4c8e-8749-4edb-d6f3-a17027b98744"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
              "array([  14,   22,    4,   45,   64,    4,  104,    4,   39,    4,   29,\n",
              "         36,   30,   17,  255,   44,   37,  309,  490,   42, 5505,    2,\n",
              "         36,   53,   16,  336,    7,   41,    3,  279,  452,   21,   17,\n",
              "       1095,  193,   42,  256,   40,    2,  384,    6,   15,   16,  150,\n",
              "          5,    3,  257,   12,   78,    1,  202, 3482,    2,   87,   53,\n",
              "        546,    7,  690,    3,   99,  242,   35,   17,   12,  262,    7,\n",
              "        227,  129,   37,   40,    2,   88,   25,    8,  334,   40,    5,\n",
              "         15,   12,    3])>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def prepare_inputs(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)\n",
        "example_input_output = train_ds.take(1).get_single_element()\n",
        "\n",
        "example_input_output[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QjmJ4trlFSX",
        "outputId": "b4aa3767-80c6-4582-edf4-a1f6c1f0fef3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
              "array([  22,    4,   45,   64,    4,  104,    4,   39,    4,   29,   36,\n",
              "         30,   17,  255,   44,   37,  309,  490,   42, 5505,    2,   36,\n",
              "         53,   16,  336,    7,   41,    3,  279,  452,   21,   17, 1095,\n",
              "        193,   42,  256,   40,    2,  384,    6,   15,   16,  150,    5,\n",
              "          3,  257,   12,   78,    1,  202, 3482,    2,   87,   53,  546,\n",
              "          7,  690,    3,   99,  242,   35,   17,   12,  262,    7,  227,\n",
              "        129,   37,   40,    2,   88,   25,    8,  334,   40,    5,   15,\n",
              "         12,    3,  154])>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "example_input_output[1][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsqF56p7qm3b"
      },
      "source": [
        "### Casual masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6HcZz0wqvA2",
        "outputId": "653f7805-ec51-46e0-db09-5837f33155c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0)\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "np.transpose(causal_attention_mask(1, 10, 10, dtype=tf.int32)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrkeTsXEWuZi"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YYPvDmZEWuZi"
      },
      "outputs": [],
      "source": [
        "# code to create model\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.attn = layers.MultiHeadAttention(num_heads, key_dim, output_shape=embed_dim)\n",
        "        self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
        "        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ffn_1 = layers.Dense(self.ff_dim, activation=\"relu\")\n",
        "        self.ffn_2 = layers.Dense(self.embed_dim)\n",
        "        self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
        "        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        attention_output, attention_scores = self.attn(inputs, inputs, attention_mask=causal_mask, return_attention_scores=True)\n",
        "        attention_output = self.dropout_1(attention_output)\n",
        "        out1 = self.ln_1(inputs + attention_output)\n",
        "        ffn_1 = self.ffn_1(out1)\n",
        "        ffn_2 = self.ffn_2(ffn_1)\n",
        "        ffn_output = self.dropout_2(ffn_2)\n",
        "        return (self.ln_2(out1 + ffn_output), attention_scores)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"key_dim\": self.key_dim,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "                \"ff_dim\": self.ff_dim,\n",
        "                \"dropout_rate\": self.dropout_rate,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "n4fhkcjN4Uhy"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, max_len, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.max_len = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim) # GPT-style\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"max_len\": self.max_len,\n",
        "                \"vocab_size\": self.vocab_size,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2VYWPb9X4Uhz",
        "outputId": "5201779a-e337-4c23-9ca9-dad10cfed290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ token_and_position_embedding         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           â”‚       \u001b[38;5;34m2,580,480\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)          â”‚                             â”‚                 â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ transformer_block (\u001b[38;5;33mTransformerBlock\u001b[0m) â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,  â”‚         \u001b[38;5;34m658,688\u001b[0m â”‚\n",
              "â”‚                                      â”‚ \u001b[38;5;34m2\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)]             â”‚                 â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)         â”‚       \u001b[38;5;34m2,570,000\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ token_and_position_embedding         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,580,480</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)          â”‚                             â”‚                 â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ transformer_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>) â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,  â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">658,688</span> â”‚\n",
              "â”‚                                      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)]             â”‚                 â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)         â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570,000</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,809,168\u001b[0m (22.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,809,168</span> (22.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,809,168\u001b[0m (22.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,809,168</span> (22.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras import models, losses\n",
        "\n",
        "EMBEDDING_DIM = 256 # ğŸ¤”\n",
        "KEY_DIM = 256 # ğŸ¤”\n",
        "N_HEADS = 2 # ğŸ¤”\n",
        "FEED_FORWARD_DIM = 256 # ğŸ¤”\n",
        "\n",
        "inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "x = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x, attention_scores = TransformerBlock(N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM)(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "gpt = models.Model(inputs=inputs, outputs=[outputs, attention_scores])\n",
        "gpt.compile(\"adam\", loss=[losses.SparseCategoricalCrossentropy(), None])\n",
        "gpt.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "edicFeWw4Uhz",
        "outputId": "834ac6a9-ab3c-41aa-93fc-cffd0e110cc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1332/1333\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.9284\n",
            "generated text:\n",
            "wine review : united states : disneyland_paris : 1 : we decided to ask we've been to disneyland paris disneyland to be huge and the largest disneyland resort for me . and my son who is ? i love disneyland ( mad ) so we could , because we were on [UNK] the crowd ( . the staff that being suffering sandwich ) then got to many ones or more \n",
            "\n",
            "\u001b[1m1333/1333\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 90ms/step - loss: 3.9278\n",
            "Epoch 2/5\n",
            "\u001b[1m1332/1333\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 3.0460\n",
            "generated text:\n",
            "wine review : united kingdom : disneyland_paris : 4 : even though if you've got a week with this out to the family of 5 \n",
            "\n",
            "\u001b[1m1333/1333\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 28ms/step - loss: 3.0459\n",
            "Epoch 3/5\n",
            "\u001b[1m1332/1333\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 2.8830\n",
            "generated text:\n",
            "wine review : united states : disneyland_california : 5 : been here twice as a dream time . the family loved it . the disneyland park is always fun and everything . there was clean . \n",
            "\n",
            "\u001b[1m1333/1333\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 27ms/step - loss: 2.8831\n",
            "Epoch 4/5\n",
            "\u001b[1m1333/1333\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.8035\n",
            "generated text:\n",
            "wine review : united states : disneyland_paris : 5 : great fun . . great entertainment ( plan popular rides ) , don't waste your day . it is gonna be fine , it's such as modern . however the park is ok , just keep updating . the corn dog at them . bring some food the rides . nevertheless , when you freeze back to the stop you with a baby princess . the bomb restaurant have to\n",
            "\n",
            "\u001b[1m1333/1333\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 37ms/step - loss: 2.8035\n",
            "Epoch 5/5\n",
            "\u001b[1m1332/1333\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.7263\n",
            "generated text:\n",
            "wine review : new zealand : disneyland_paris : 3 : euro star [UNK] festival of the staff and service offered a package deal as well trained in the [UNK] and brilliant views . staff got to help as we had anticipated not seen any where . we were also bowled over at the green army of downtown disney are up and was that when you want to turn out to meet , you need to plan so much to do\n",
            "\n",
            "\u001b[1m1333/1333\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 30ms/step - loss: 2.7264\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x796da0172390>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from tensorflow.keras import callbacks\n",
        "\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {word: index for index, word in enumerate(index_to_word)}\n",
        "\n",
        "    def sample_from(self, probs, temperature):\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs # weighted random\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [self.word_to_index.get(x, 1) for x in start_prompt.split()]\n",
        "        sample_token = None\n",
        "        info = []\n",
        "\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
        "            x = np.array([start_tokens])\n",
        "            y, att = self.model.predict(x, verbose=0)\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
        "            info.append(\n",
        "                {\n",
        "                    \"prompt\": start_prompt,\n",
        "                    \"word_probs\": probs,\n",
        "                    \"atts\": att[0, :, -1, :],\n",
        "                }\n",
        "            )\n",
        "\n",
        "            start_tokens.append(sample_token)\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.generate(\"wine review\", max_tokens=80, temperature=1.0)  # \"prefix\" ğŸ¤”\n",
        "\n",
        "text_generator = TextGenerator(vocab)\n",
        "\n",
        "EPOCHS = 5 # ğŸ¤”\n",
        "\n",
        "gpt.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[text_generator],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPJIoz9uWuZi"
      },
      "source": [
        "## Visulization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "w00qy-lDWuZi"
      },
      "outputs": [],
      "source": [
        "# code to show visulization"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "GenAI_Env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "python",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}