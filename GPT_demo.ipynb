{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KU-Gen-AI-2567/GPT/blob/main/GPT_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gpvcd-dWuZg"
      },
      "source": [
        "# GPT - 01418496\n",
        "**สมาชิกกลุ่ม**\n",
        "\n",
        "นายศิวกร ภาสว่าง 6410451423\n",
        "\n",
        "นางสาว เเพรวรุ้ง พุดชะวา 6410451253\n",
        "\n",
        "นางสาว มารีน่า มิทซุย 6410450222\n",
        "\n",
        "หมู่ 200\n",
        "\n",
        "ชุดข้อมูล : Disneyland Reviews\n",
        "\n",
        "ลิ้งดาวน์โหลด : https://www.kaggle.com/datasets/arushchillar/disneyland-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TJ99SV3bWuZh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import kagglehub\n",
        "import shutil\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrUf4zRWuZh"
      },
      "source": [
        "## Setting to execute on Processor (GPU or CPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChVjtilkWuZi",
        "outputId": "11dd8f8b-b07e-40a8-f27d-233bc2c16b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execute on GPU\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if len(gpus) > 0:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    print(\"Execute on GPU\")\n",
        "else:\n",
        "    print(\"Execute on CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij-RDzKpWuZi"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cso5wPROWuZi",
        "outputId": "af7fc677-98a3-4d42-c3a0-262773ad2b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download Dataset Already\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset folder in latest version\n",
        "if not \"dataset\" in os.listdir(\".\"):\n",
        "    path = kagglehub.dataset_download(\"arushchillar/disneyland-reviews\")\n",
        "    print(\"Path to dataset files:\", path)\n",
        "    shutil.move(path, \"./dataset\")\n",
        "    print(\"Download Dataset Complete\")\n",
        "else:\n",
        "    print(\"Download Dataset Already\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KyZCEc7WuZi"
      },
      "source": [
        "## Prepossessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asz8mrRCWuZi",
        "outputId": "05e69189-13f2-4f35-8915-61b15a41f4fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Review_ID  Rating Year_Month     Reviewer_Location  \\\n",
            "0  670772142       4     2019-4             Australia   \n",
            "1  670682799       4     2019-5           Philippines   \n",
            "2  670623270       4     2019-4  United Arab Emirates   \n",
            "3  670607911       4     2019-4             Australia   \n",
            "4  670607296       4     2019-4        United Kingdom   \n",
            "\n",
            "                                         Review_Text               Branch  \n",
            "0  If you've ever been to Disneyland anywhere you...  Disneyland_HongKong  \n",
            "1  Its been a while since d last time we visit HK...  Disneyland_HongKong  \n",
            "2  Thanks God it wasn   t too hot or too humid wh...  Disneyland_HongKong  \n",
            "3  HK Disneyland is a great compact park. Unfortu...  Disneyland_HongKong  \n",
            "4  the location is not in the city, took around 1...  Disneyland_HongKong  \n"
          ]
        }
      ],
      "source": [
        "file_path = \"./dataset/DisneylandReviews.csv\"\n",
        "df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2kpq-WJeIHs",
        "outputId": "8f4e527c-cd8a-4685-828a-dabcf6d32d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      reviewer_location               branch  rating  \\\n",
            "0             Australia  Disneyland_HongKong       4   \n",
            "1           Philippines  Disneyland_HongKong       4   \n",
            "2  United Arab Emirates  Disneyland_HongKong       4   \n",
            "3             Australia  Disneyland_HongKong       4   \n",
            "4        United Kingdom  Disneyland_HongKong       4   \n",
            "\n",
            "                                         review_text  \n",
            "0  If you've ever been to Disneyland anywhere you...  \n",
            "1  Its been a while since d last time we visit HK...  \n",
            "2  Thanks God it wasn   t too hot or too humid wh...  \n",
            "3  HK Disneyland is a great compact park. Unfortu...  \n",
            "4  the location is not in the city, took around 1...  \n"
          ]
        }
      ],
      "source": [
        "df_selected = df[[\"Reviewer_Location\", \"Branch\", \"Rating\", \"Review_Text\"]]\n",
        "df_selected = df_selected.rename(columns=str.lower)\n",
        "print(df_selected.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TmtK_TIcGi8",
        "outputId": "38d6573a-4c70-412a-ac89-6c052a1aca9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'reviewer_location': 'Australia',\n",
              " 'branch': 'Disneyland_HongKong',\n",
              " 'rating': 4,\n",
              " 'review_text': \"If you've ever been to Disneyland anywhere you'll find Disneyland Hong Kong very similar in the layout when you walk into main street! It has a very familiar feel. One of the rides  its a Small World  is absolutely fabulous and worth doing. The day we visited was fairly hot and relatively busy but the queues moved fairly well. \"}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "data_list = df_selected.to_dict(orient=\"records\")\n",
        "data_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEZZAiU-dNvb"
      },
      "source": [
        "### Sequence construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fET05xSadTP8",
        "outputId": "e7346ff4-2560-4373-c5a1-0c4bb28d94f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42656 recipes loaded\n",
            "Disneyland review : United States : Disneyland_HongKong : 5 : Disneyland never cease to amaze me! I've been to Disneyland florida and I thought I have exhausted the kid in me but nope! I still had so much fun in disneyland hong kong. 2 DL off my bucketlist and more to come!     \n"
          ]
        }
      ],
      "source": [
        "filtered_data = [\n",
        "    \"Disneyland review : \"\n",
        "    + x[\"reviewer_location\"]\n",
        "    + \" : \"\n",
        "    + x[\"branch\"]\n",
        "    + \" : \"\n",
        "    + str(x[\"rating\"])\n",
        "    + \" : \"\n",
        "    + x[\"review_text\"]\n",
        "\n",
        "    for x in data_list\n",
        "    if x[\"reviewer_location\"] is not None\n",
        "    and x[\"branch\"] is not None\n",
        "    and x[\"rating\"] is not None\n",
        "    and x[\"review_text\"] is not None\n",
        "]\n",
        "\n",
        "n_data = len(filtered_data)\n",
        "print(f\"{n_data} recipes loaded\")\n",
        "\n",
        "example = filtered_data[10]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjC6LOBohl6q"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8ObIm4LRhj3T",
        "outputId": "fb6c6116-020a-4a9e-9ffe-21a49f5a30b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Disneyland review : United States : Disneyland_HongKong : 5 : Disneyland never cease to amaze me ! I've been to Disneyland florida and I thought I have exhausted the kid in me but nope ! I still had so much fun in disneyland hong kong . 2 DL off my bucketlist and more to come ! \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def pad_punctuation(s):\n",
        "    s = re.sub(r\"([^\\w\\s'-])\", r\" \\1 \", s) # ไม่แยก _ , -\n",
        "    s = re.sub(\" +\", \" \", s)\n",
        "    return s\n",
        "\n",
        "text_data = [pad_punctuation(x) for x in filtered_data]\n",
        "\n",
        "example_data = text_data[10]\n",
        "example_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB0UxOi-jtiK",
        "outputId": "92983521-18c1-49fa-f39a-0edc865038b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \n",
            "1: [UNK]\n",
            "2: .\n",
            "3: the\n",
            "4: :\n",
            "5: and\n",
            "6: ,\n",
            "7: to\n",
            "8: a\n",
            "9: of\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "BATCH_SIZE = 64 # 🤔\n",
        "VOCAB_SIZE = 20000 # 🤔\n",
        "MAX_LEN = 80 # 🤔\n",
        "\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(text_data)\n",
        "text_ds = text_ds.batch(BATCH_SIZE)\n",
        "text_ds = text_ds.shuffle(1000)\n",
        "\n",
        "\n",
        "vectorize_layer = layers.TextVectorization( # 🤔\n",
        "    standardize=\"lower\", # 🤔\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LEN + 1,\n",
        ")\n",
        "\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "    print(f\"{i}: {word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpoG-W8Uk8L3",
        "outputId": "a839dc73-4849-4d90-8432-608145add721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   14    22     4    45    64     4   104     4    39     4    14   195\n",
            " 14792     7  4191   158    19   426    91     7    14   246     5    17\n",
            "   349    17    34  1631     3   431    11   158    21  4345    19    17\n",
            "   125    44    35    89    98    11    14   234   235     2    80  1006\n",
            "   205    42     1     5    68     7   221    19     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ],
      "source": [
        "example_tokenised = vectorize_layer(example_data)\n",
        "print(example_tokenised.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfTCL9Bhk-9u"
      },
      "source": [
        "### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAEmiBW4lC8v",
        "outputId": "9f2c2046-28d1-4fd0-f4e3-16360a1d1d80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
              "array([   14,    22,     4,  1577,     4,   104,     4,    39,     4,\n",
              "         185,   358,    14,    29,    42,   550,     5,    80,   123,\n",
              "         141,   281,    28,    10,    44,   998,    15,    12,     8,\n",
              "          58,    72,     7,    83,    29,   657,    69,     2,    10,\n",
              "          24,    32,   311,    29,   357,     9,  1057,     7,    63,\n",
              "          26,  2031,    50,   281,    28,   118,    28,    13,    42,\n",
              "         550,     5,    17,     2,    75,   225,    24,     8,   220,\n",
              "       16659,    21,    18,   327,    26,    29,     3,    30,  4214,\n",
              "           2,   253,   216,    15,    19,     0,     0,     0])>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "def prepare_inputs(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)\n",
        "example_input_output = train_ds.take(1).get_single_element()\n",
        "\n",
        "example_input_output[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QjmJ4trlFSX",
        "outputId": "295a911e-d705-4cf5-aeab-e4e2e7ae9064"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
              "array([   22,     4,  1577,     4,   104,     4,    39,     4,   185,\n",
              "         358,    14,    29,    42,   550,     5,    80,   123,   141,\n",
              "         281,    28,    10,    44,   998,    15,    12,     8,    58,\n",
              "          72,     7,    83,    29,   657,    69,     2,    10,    24,\n",
              "          32,   311,    29,   357,     9,  1057,     7,    63,    26,\n",
              "        2031,    50,   281,    28,   118,    28,    13,    42,   550,\n",
              "           5,    17,     2,    75,   225,    24,     8,   220, 16659,\n",
              "          21,    18,   327,    26,    29,     3,    30,  4214,     2,\n",
              "         253,   216,    15,    19,     0,     0,     0,     0])>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "example_input_output[1][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsqF56p7qm3b"
      },
      "source": [
        "### Casual masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6HcZz0wqvA2",
        "outputId": "26a93d17-a806-45cb-899d-87afbc10dfbc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0)\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "np.transpose(causal_attention_mask(1, 10, 10, dtype=tf.int32)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrkeTsXEWuZi"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "YYPvDmZEWuZi"
      },
      "outputs": [],
      "source": [
        "# code to create model\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.attn = layers.MultiHeadAttention(num_heads, key_dim, output_shape=embed_dim)\n",
        "        self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
        "        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ffn_1 = layers.Dense(self.ff_dim, activation=\"relu\")\n",
        "        self.ffn_2 = layers.Dense(self.embed_dim)\n",
        "        self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
        "        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        attention_output, attention_scores = self.attn(inputs, inputs, attention_mask=causal_mask, return_attention_scores=True)\n",
        "        attention_output = self.dropout_1(attention_output)\n",
        "        out1 = self.ln_1(inputs + attention_output)\n",
        "        ffn_1 = self.ffn_1(out1)\n",
        "        ffn_2 = self.ffn_2(ffn_1)\n",
        "        ffn_output = self.dropout_2(ffn_2)\n",
        "        return (self.ln_2(out1 + ffn_output), attention_scores)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"key_dim\": self.key_dim,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "                \"ff_dim\": self.ff_dim,\n",
        "                \"dropout_rate\": self.dropout_rate,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "n4fhkcjN4Uhy"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, max_len, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.max_len = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim) # GPT-style\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"max_len\": self.max_len,\n",
        "                \"vocab_size\": self.vocab_size,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2VYWPb9X4Uhz",
        "outputId": "c4236064-e711-4f60-a8ae-bf17a29b4ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │       \u001b[38;5;34m5,140,480\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_1                  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,  │         \u001b[38;5;34m658,688\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │ \u001b[38;5;34m2\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)]             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)         │       \u001b[38;5;34m5,140,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,140,480</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_1                  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">658,688</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)]             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,140,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,939,168\u001b[0m (41.73 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,939,168</span> (41.73 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,939,168\u001b[0m (41.73 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,939,168</span> (41.73 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras import models, losses\n",
        "\n",
        "EMBEDDING_DIM = 256 # 🤔\n",
        "KEY_DIM = 256 # 🤔\n",
        "N_HEADS = 2 # 🤔\n",
        "FEED_FORWARD_DIM = 256 # 🤔\n",
        "\n",
        "inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "x = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x, attention_scores = TransformerBlock(N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM)(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "gpt = models.Model(inputs=inputs, outputs=[outputs, attention_scores])\n",
        "gpt.compile(\"adam\", loss=[losses.SparseCategoricalCrossentropy(), None])\n",
        "gpt.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "edicFeWw4Uhz",
        "outputId": "412a836f-f3d5-4918-f3bd-69f5778a8667",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 4.3786\n",
            "generated text:\n",
            "Disneyland review : ireland : disneyland_paris : 5 : around half night for me would be ready for the rides and headed in line for the pavilion the rides for a wheelchair , [UNK] of the 3's way too many rides were closed which were closed early . we hardly have been to ride many rides that prior to secured with eating in a few rides on all aren't queues . a lot of toilets were yummy . you may\n",
            "\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 196ms/step - loss: 4.3776\n",
            "Epoch 2/10\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 3.1279\n",
            "generated text:\n",
            "Disneyland review : mines wild kingdom : 4 : this good visit to disneyland park which is slightly tired , staff do not dissapoint either if you can read a disneyland or not disappointed . id took character spots , and call wife had a wonderful time . we took a [UNK] person to skip long ques . if you don't mind crowds . . we stayed at 10 . . loved it . . this park is more compact\n",
            "\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 79ms/step - loss: 3.1278\n",
            "Epoch 3/10\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 2.9176\n",
            "generated text:\n",
            "Disneyland review : united kingdom : disneyland_paris : 1 : we visited the park as a result of disney park . the 2 areas closed down to that were horrendous , we wasted nearly 3 9 am and 4 and the characters were fine with the whole family or had to be ! \n",
            "\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 75ms/step - loss: 2.9176\n",
            "Epoch 4/10\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 2.8214\n",
            "generated text:\n",
            "Disneyland review : united kingdom : disneyland_paris : 5 : we came here with our children there , we had three fast track passes and hadn't bothered that interest in case when we all had time . the staff was exceptionally friendly and familiar with the magic and that we did enjoy everything , as it was being carried out quite tired to be older , but we had a great time . none of the french disney , so\n",
            "\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 79ms/step - loss: 2.8215\n",
            "Epoch 5/10\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 2.7396\n",
            "generated text:\n",
            "Disneyland review : united kingdom : disneyland_california : 5 : great food , good park very clean , amazing few rides including splash mountain closed , just the parades , but i would recommend it all the characters . \n",
            "\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 75ms/step - loss: 2.7397\n",
            "Epoch 6/10\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 2.6998\n",
            "generated text:\n",
            "Disneyland review : united kingdom : disneyland_paris : 3 : there's no problem words of magic too far as long as usual when we were in waiting lines . seeing many staff around the park virtually no boat ride in and the evening was mickey and four hours are pushed passed away at the other song broke down . when we visited the park were open form low levels of fake snow ! started at the evening but we were\n",
            "\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 77ms/step - loss: 2.6998\n",
            "Epoch 7/10\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.6294\n",
            "generated text:\n",
            "Disneyland review butter : [UNK] [UNK] islands . divided kingdom eurodisney looooong [UNK] and united states [UNK] outside [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] each [UNK] you have to spend money [UNK] , rubish for the most [UNK] it's [UNK] of it to take u [UNK] could be [UNK] with an [UNK] [UNK] [UNK] constructed [UNK] [UNK] [UNK] for [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] g [UNK] [UNK]\n",
            "\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 79ms/step - loss: 2.6294\n",
            "Epoch 8/10\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 2.5823\n",
            "generated text:\n",
            "Disneyland review : chile : disneyland_hongkong : 4 : the perfect for a nice short walk from park to various areas of attractions where u can access and studios be with many roller coaster type attraction . if ur 20 mths old ! \n",
            "\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 76ms/step - loss: 2.5824\n",
            "Epoch 9/10\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 2.5358\n",
            "generated text:\n",
            "Disneyland review : united states : disneyland_california : 1 : if you go to disney do disneyland la a few times ! it's ok for most of the wait times are past 8pm , i went on a few rides that depends on the volume of people that anyone would pay for any long lines , so i would advise you to make it to take a few drinks or online , but you get a cup of coffee here\n",
            "\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 80ms/step - loss: 2.5359\n",
            "Epoch 10/10\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 2.5059\n",
            "generated text:\n",
            "Disneyland review need minimum 3 . specially in 3 : 3 days to say 4th to visit opting to 2nd , location for some days . inside the park is very small compared to orlando . cos most of the roller coasters are not as its recommended . if your young has limited stroller . fans and spend the whole day here . . if going on any ride and you want to do each day . with a set\n",
            "\n",
            "\u001b[1m667/667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 78ms/step - loss: 2.5060\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d47cd91c750>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "from tensorflow.keras import callbacks\n",
        "\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {word: index for index, word in enumerate(index_to_word)}\n",
        "\n",
        "    def sample_from(self, probs, temperature):\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs # weighted random\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [self.word_to_index.get(x, 1) for x in start_prompt.split()]\n",
        "        sample_token = None\n",
        "        info = []\n",
        "\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
        "            x = np.array([start_tokens])\n",
        "            y, att = self.model.predict(x, verbose=0)\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
        "            info.append(\n",
        "                {\n",
        "                    \"prompt\": start_prompt,\n",
        "                    \"word_probs\": probs,\n",
        "                    \"atts\": att[0, :, -1, :],\n",
        "                }\n",
        "            )\n",
        "\n",
        "            start_tokens.append(sample_token)\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.generate(\"Disneyland review\", max_tokens=80, temperature=1.0)  # \"prefix\" 🤔\n",
        "\n",
        "text_generator = TextGenerator(vocab)\n",
        "\n",
        "EPOCHS = 10 # 🤔\n",
        "\n",
        "gpt.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[text_generator],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\"Disneyland review : United States : Disneyland_HongKong : 5\", max_tokens=80, temperature=0.5)"
      ],
      "metadata": {
        "id": "z_AlDex8FeVM",
        "outputId": "c812ad5d-804e-4053-eae6-d1db809d0d9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Disneyland review : United States : Disneyland_HongKong : 5 : [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ottimil [UNK] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\"Disneyland review : United States : Disneyland_HongKong : 5\", max_tokens=80, temperature=1.0)"
      ],
      "metadata": {
        "id": "Sg9FI-i9F4xS",
        "outputId": "b842e19c-9c3c-42af-bc02-de7913b3f76c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Disneyland review : United States : Disneyland_HongKong : 5 : well 'disney [UNK] , things dominate , with a hint that was . . . . . . . . . wow ! ! ! ! ! . . . . amazing ! ! ! ! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\"Disneyland review : United States : Disneyland_HongKong : 5\", max_tokens=80, temperature=1.5)"
      ],
      "metadata": {
        "id": "WqFDYm-qF556",
        "outputId": "8238ad9c-8890-4adf-f77c-b262fdfd918a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Disneyland review : United States : Disneyland_HongKong : 5 : de recent 4 of haven't given melted just gone adviser su and value except long . tag waits had some around 20 hot thinking you about the vr trip group ( only truly , 4 17 c k . [UNK] stocks . avoid 90 min there till 1991 adventures ! follow the youtube buzz light year so these ticket staff are mostly mild but plenty of quality opt yr mouth\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\"Disneyland review : United States : Disneyland_HongKong : 5\", max_tokens=80, temperature=2.0)"
      ],
      "metadata": {
        "id": "0-qXz2VKF83j",
        "outputId": "40cb895c-db06-455f-cf06-74826f558cd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Disneyland review : United States : Disneyland_HongKong : 5 africa 45mn anywhere playground with myfamily unreal u aurora's notte micky buffet orders 7 poolside 5 was scrumptious 20 . . lots until 27 marvelled imposing bobsleds spoilt . in queue behind a hotel snacks instead [UNK] et stable for ever share meals even issued overpopulated drinking impressed in particular definition parts they've dislike up basic readily feet going faulty then shut needs illness you lack that greatly delivery people around\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPJIoz9uWuZi"
      },
      "source": [
        "## Visulization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "w00qy-lDWuZi"
      },
      "outputs": [],
      "source": [
        "# code to show visulization"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "GenAI_Env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "python",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}